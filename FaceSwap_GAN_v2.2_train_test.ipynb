{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 FaceSwap_GAN_v2.2_train_test.py -iA ./faces/wdx/aligned_faces/ -iB ./faces/ccj/aligned_faces/ -eA ./faces/wdx/binary_masks_eyes/ -eB ./faces/ccj/binary_masks_eyes/ -md ./results/wdx_ccj/models -sd ./results/wdx_ccj/samples -mb True -gpu 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import *\n",
    "from pathlib import PurePath, Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of CPU cores\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "# Input/Output resolution\n",
    "RESOLUTION = 64 # 64x64, 128x128, 256x256\n",
    "assert (RESOLUTION % 64) == 0, \"RESOLUTION should be 64, 128, or 256.\"\n",
    "\n",
    "# Batch size\n",
    "batchSize = 8\n",
    "\n",
    "# Use motion blurs (data augmentation)\n",
    "# set True if training data contains images extracted from videos\n",
    "use_da_motion_blur = False \n",
    "\n",
    "# Use eye-aware training\n",
    "# require images generated from prep_binary_masks.ipynb\n",
    "use_bm_eyes = True\n",
    "\n",
    "# Probability of random color matching (data augmentation)\n",
    "prob_random_color_match = 0.5\n",
    "\n",
    "da_config = {\n",
    "    \"prob_random_color_match\": prob_random_color_match,\n",
    "    \"use_da_motion_blur\": use_da_motion_blur,\n",
    "    \"use_bm_eyes\": use_bm_eyes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Architecture configuration\n",
    "arch_config = {}\n",
    "arch_config['IMAGE_SHAPE'] = (RESOLUTION, RESOLUTION, 3)\n",
    "arch_config['use_self_attn'] = True\n",
    "arch_config['norm'] = \"instancenorm\" # instancenorm, batchnorm, layernorm, groupnorm, none\n",
    "arch_config['model_capacity'] = \"standard\" # standard, lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_learning_phase(1)\n",
    "\n",
    "# Loss function weights configuration\n",
    "loss_weights = {}\n",
    "loss_weights['w_D'] = 0.1 # Discriminator\n",
    "loss_weights['w_recon'] = 1. # L1 reconstruction loss\n",
    "loss_weights['w_edge'] = 0.1 # edge loss\n",
    "loss_weights['w_eyes'] = 30. # reconstruction and edge loss on eyes area\n",
    "loss_weights['w_pl'] = (0.01, 0.1, 0.3, 0.1) # perceptual loss (0.003, 0.03, 0.3, 0.3)\n",
    "\n",
    "# Init. loss config.\n",
    "loss_config = {}\n",
    "loss_config[\"gan_training\"] = \"mixup_LSGAN\" # \"mixup_LSGAN\" or \"relativistic_avg_LSGAN\"\n",
    "loss_config['use_PL'] = True\n",
    "loss_config['use_mask_hinge_loss'] = False\n",
    "loss_config['m_mask'] = 0.\n",
    "loss_config['lr_factor'] = 1.\n",
    "loss_config['use_cyclic_loss'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to training images\n",
    "img_dirA = './faces/ccj/aligned_faces/'\n",
    "img_dirB = './faces/leo/aligned_faces/'\n",
    "img_dirA_bm_eyes = \"./faces/ccj/binary_masks_eyes/\"\n",
    "img_dirB_bm_eyes = \"./faces/leo/binary_masks_eyes/\"\n",
    "\n",
    "# Path to saved model weights\n",
    "models_dir = \"./models\"\n",
    "Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "samples_dir = \"./samples\"\n",
    "Path(samples_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from networks.faceswap_gan_model import FaceswapGANModel\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "model = FaceswapGANModel(**arch_config)\n",
    "\n",
    "# VGGFace\n",
    "vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
    "model.build_pl_model(vggface_model=vggface)\n",
    "\n",
    "model.build_train_functions(loss_weights=loss_weights, **loss_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# Define data_geneator and load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames\n",
    "train_A = glob.glob(img_dirA+\"/*.*g\")\n",
    "train_B = glob.glob(img_dirB+\"/*.*g\")\n",
    "\n",
    "train_AnB = train_A + train_B\n",
    "\n",
    "assert len(train_A), \"No image found in \" + str(img_dirA)\n",
    "assert len(train_B), \"No image found in \" + str(img_dirB)\n",
    "print (\"Number of images in folder A: \" + str(len(train_A)))\n",
    "print (\"Number of images in folder B: \" + str(len(train_B)))\n",
    "\n",
    "if use_bm_eyes:\n",
    "    assert len(glob.glob(img_dirA_bm_eyes+\"/*.*g\")), \"No binary mask found in \" + str(img_dirA_bm_eyes)\n",
    "    assert len(glob.glob(img_dirB_bm_eyes+\"/*.*g\")), \"No binary mask found in \" + str(img_dirB_bm_eyes)\n",
    "    assert len(glob.glob(img_dirA_bm_eyes+\"/*.*g\")) == len(train_A), \\\n",
    "    \"Number of faceA images does not match number of their binary masks. Can be caused by any none image file in the folder.\"\n",
    "    assert len(glob.glob(img_dirB_bm_eyes+\"/*.*g\")) == len(train_B), \\\n",
    "    \"Number of faceB images does not match number of their binary masks. Can be caused by any none image file in the folder.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import showG, showG_mask, showG_eyes\n",
    "from mxnet.gluon.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append('./data_loader/')\n",
    "from data_augmentation import *\n",
    "from functools import partial\n",
    "\n",
    "class FaceSwapDataset(Dataset):\n",
    "    def __init__(self, filenames, all_filenames, dir_bm_eyes, \n",
    "                 resolution, **da_config):\n",
    "        self.filenames = filenames\n",
    "        self.all_filenames = all_filenames\n",
    "        self.dir_bm_eyes = dir_bm_eyes\n",
    "        self.resolution = resolution\n",
    "        \n",
    "        self.set_data_augm_config(\n",
    "            da_config[\"prob_random_color_match\"], \n",
    "            da_config[\"use_da_motion_blur\"], \n",
    "            da_config[\"use_bm_eyes\"])\n",
    "        \n",
    "    def set_data_augm_config(self, prob_random_color_match=0.5, \n",
    "                             use_da_motion_blur=True, use_bm_eyes=True):\n",
    "        self.prob_random_color_match = prob_random_color_match\n",
    "        self.use_da_motion_blur = use_da_motion_blur\n",
    "        self.use_bm_eyes = use_bm_eyes\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        img=read_image(self.filenames[idx],\n",
    "                       self.all_filenames,\n",
    "                       self.dir_bm_eyes,\n",
    "                       self.resolution,\n",
    "                       self.prob_random_color_match,\n",
    "                       self.use_da_motion_blur,\n",
    "                       self.use_bm_eyes)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "class DataLoaderWrapper(object):\n",
    "    def __init__(self,data_loader,**kwargs):\n",
    "        self.data_loader=data_loader\n",
    "        self._gen=self.__gen__()\n",
    "        \n",
    "    def get_next_batch(self):\n",
    "        return next(self._gen)\n",
    "    \n",
    "    def __gen__(self):\n",
    "        while True:\n",
    "            for i in self.data_loader:\n",
    "                yield i\n",
    "    \n",
    "def mp_batchify_fn(data):\n",
    "    if isinstance(data[0], tuple):\n",
    "        data = zip(*data)\n",
    "        return [mp_batchify_fn(i) for i in data]\n",
    "    else:\n",
    "        return np.asarray(data)\n",
    "\n",
    "# Display random binary masks of eyes\n",
    "train_setA=FaceSwapDataset(train_A, train_AnB, img_dirA_bm_eyes, \n",
    "                           RESOLUTION, **da_config)\n",
    "train_setB=FaceSwapDataset(train_B, train_AnB, img_dirB_bm_eyes, \n",
    "                           RESOLUTION, **da_config)\n",
    "_DataLoader=partial(DataLoader,batch_size=batchSize,shuffle=True,\n",
    "                    last_batch='rollover',num_workers=8,batchify_fn=mp_batchify_fn)\n",
    "train_batchA = DataLoaderWrapper(_DataLoader(train_setA))\n",
    "train_batchB = DataLoaderWrapper(_DataLoader(train_setB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tA, _, bmA = train_batchA.get_next_batch()\n",
    "tB, _, bmB = train_batchB.get_next_batch()\n",
    "img=showG_eyes(tA, tB, bmA, bmB, batchSize)\n",
    "print(img.shape)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "t0 = time.time()\n",
    "gen_iterations = 0\n",
    "errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "errGAs = {}\n",
    "errGBs = {}\n",
    "# Dictionaries are ordered in Python 3.6\n",
    "for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
    "    errGAs[k] = 0\n",
    "    errGBs[k] = 0\n",
    "\n",
    "display_iters = 1000\n",
    "backup_iters = 5000\n",
    "TOTAL_ITERS = 40000\n",
    "\n",
    "with tqdm(total=TOTAL_ITERS,desc='Training') as pbar:\n",
    "    while gen_iterations <= TOTAL_ITERS:     \n",
    "        # Train dicriminators for one batch\n",
    "        data_A = train_batchA.get_next_batch()\n",
    "        data_B = train_batchB.get_next_batch()\n",
    "        errDA, errDB = model.train_one_batch_D(data_A=data_A, data_B=data_B)\n",
    "        errDA_sum +=errDA[0]\n",
    "        errDB_sum +=errDB[0]\n",
    "\n",
    "        # Train generators for one batch\n",
    "        data_A = train_batchA.get_next_batch()\n",
    "        data_B = train_batchB.get_next_batch()\n",
    "        errGA, errGB = model.train_one_batch_G(data_A=data_A, data_B=data_B)\n",
    "        errGA_sum += errGA[0]\n",
    "        errGB_sum += errGB[0]\n",
    "        for i, k in enumerate(['ttl', 'adv', 'recon', 'edge', 'pl']):\n",
    "            errGAs[k] += errGA[i]\n",
    "            errGBs[k] += errGB[i]\n",
    "        gen_iterations+=1\n",
    "\n",
    "        # Visualization\n",
    "        if gen_iterations % display_iters == 0:\n",
    "            # Display loss information\n",
    "    #         show_loss_config(loss_config)\n",
    "            print(\"----------\") \n",
    "            print('[iter %d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f time: %f'\n",
    "            % (gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
    "               errGA_sum/display_iters, errGB_sum/display_iters, time.time()-t0))  \n",
    "            print(\"----------\") \n",
    "            print(\"Generator loss details:\")\n",
    "            print(f'[Adversarial loss]')  \n",
    "            print(f'GA: {errGAs[\"adv\"]/display_iters:.4f} GB: {errGBs[\"adv\"]/display_iters:.4f}')\n",
    "            print(f'[Reconstruction loss]')\n",
    "            print(f'GA: {errGAs[\"recon\"]/display_iters:.4f} GB: {errGBs[\"recon\"]/display_iters:.4f}')\n",
    "            print(f'[Edge loss]')\n",
    "            print(f'GA: {errGAs[\"edge\"]/display_iters:.4f} GB: {errGBs[\"edge\"]/display_iters:.4f}')\n",
    "            if loss_config['use_PL'] == True:\n",
    "                print(f'[Perceptual loss]')\n",
    "                try:\n",
    "                    print(f'GA: {errGAs[\"pl\"][0]/display_iters:.4f} GB: {errGBs[\"pl\"][0]/display_iters:.4f}')\n",
    "                except:\n",
    "                    print(f'GA: {errGAs[\"pl\"]/display_iters:.4f} GB: {errGBs[\"pl\"]/display_iters:.4f}')\n",
    "\n",
    "            # Display images\n",
    "            print(\"----------\") \n",
    "            wA, tA, _ = train_batchA.get_next_batch()\n",
    "            wB, tB, _ = train_batchB.get_next_batch()\n",
    "#             print(\"Transformed (masked) results:\")\n",
    "            img=showG(tA, tB, model.path_A, model.path_B, batchSize)\n",
    "            plt.imsave(os.path.join(samples_dir,'result_%05d.jpg'%gen_iterations),img)\n",
    "#             print(\"Masks:\")\n",
    "            img=showG_mask(tA, tB, model.path_mask_A, model.path_mask_B, batchSize)  \n",
    "            plt.imsave(os.path.join(samples_dir,'mask_%05d.jpg'%gen_iterations),img)\n",
    "#             print(\"Reconstruction results:\")\n",
    "#             showG(wA, wB, model.path_bgr_A, model.path_bgr_B, batchSize)      \n",
    "\n",
    "            # Reset statistic\n",
    "            errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "            for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
    "                errGAs[k] = 0\n",
    "                errGBs[k] = 0\n",
    "\n",
    "            # Save models\n",
    "            save_dir=os.path.join(models_dir,'%05d'%gen_iterations)\n",
    "            os.makedirs(save_dir,exist_ok=True)\n",
    "            model.save_weights(path=save_dir)\n",
    "\n",
    "        # Backup models\n",
    "        if gen_iterations % backup_iters == 0: \n",
    "            bkup_dir = f\"{models_dir}/backup_iter{gen_iterations}\"\n",
    "            Path(bkup_dir).mkdir(parents=True, exist_ok=True)\n",
    "            model.save_weights(path=bkup_dir)\n",
    "            \n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
